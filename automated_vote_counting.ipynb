{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up for kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data can be found at [Kaggle](https://www.kaggle.com/competitions/automatic-vote-counting-image)\n",
        "Add the \".kaggle.json\" file in the root directory and run the nootbook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUV8v9OmPrVN",
        "outputId": "aec17d29-4bbf-40bf-e965-290ac39390a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                 title                                  size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------  ------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "ruchi798/data-science-job-salaries                  Data Science Job Salaries               7KB  2022-06-15 08:59:12          18456        576  1.0              \n",
            "nancyalaswad90/diamonds-prices                      Diamonds Prices                       711KB  2022-07-09 14:59:21           2135         94  1.0              \n",
            "aravindas01/monkeypox-cases-countrywise-data        MonkeyPox Cases_Countrywise Data        6KB  2022-08-10 17:12:36            602         33  0.9117647        \n",
            "faryarmemon/usa-housing-market-factors              U.S. Housing Market Factors            32KB  2022-08-03 02:19:31            473         33  1.0              \n",
            "himanshunakrani/student-study-hours                 Student Study Hours                    276B  2022-07-20 13:17:29           1634         63  1.0              \n",
            "zzettrkalpakbal/full-filled-brain-stroke-dataset    Brain stroke prediction dataset        52KB  2022-07-16 09:57:08           2096         65  0.9705882        \n",
            "jillanisofttech/brain-stroke-dataset                Brain Stroke Dataset                   47KB  2022-08-04 18:02:56            734         30  0.9705882        \n",
            "erqizhou/students-data-analysis                     Students Data Analysis                  2KB  2022-07-20 03:54:13            886         32  1.0              \n",
            "dansbecker/melbourne-housing-snapshot               Melbourne Housing Snapshot            451KB  2018-06-05 12:52:24          92915       1119  0.7058824        \n",
            "gabrielabilleira/football-manager-2022-player-data  Football Manager 2022 Player Data      94KB  2022-07-26 09:49:50            579         29  1.0              \n",
            "mukuldeshantri/ecommerce-fashion-dataset            E-commerce Dataset with 30K Products  546KB  2022-07-08 12:28:18           2371         69  1.0              \n",
            "datasnaek/youtube-new                               Trending YouTube Video Statistics     201MB  2019-06-03 00:56:47         181209       4630  0.7941176        \n",
            "zynicide/wine-reviews                               Wine Reviews                           51MB  2017-11-27 17:08:04         164324       3350  0.7941176        \n",
            "himanshunakrani/cryptocurrencies-dataset            Cryptocurrency Prices Dataset         156KB  2022-08-11 17:46:17            471         25  1.0              \n",
            "residentmario/ramen-ratings                         Ramen Ratings                          40KB  2018-01-11 16:04:39          35126        804  0.7058824        \n",
            "rtatman/188-million-us-wildfires                    1.88 Million US Wildfires             168MB  2020-05-12 21:03:49          21007       1027  0.8235294        \n",
            "datasnaek/chess                                     Chess Game Dataset (Lichess)            3MB  2017-09-04 03:09:09          30808       1026  0.8235294        \n",
            "jpmiller/publicassistance                           US Public Food Assistance             703KB  2020-08-21 16:51:18          16844        402  0.9117647        \n",
            "dansbecker/powerlifting-database                    powerlifting-database                   9MB  2019-04-30 21:07:41           5238         65  0.5882353        \n",
            "nasa/kepler-exoplanet-search-results                Kepler Exoplanet Search Results         1MB  2017-10-10 18:26:59          10725        668  0.8235294        \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "!pip install -q kaggle\n",
        "! mkdir ~/.kaggle \n",
        "! cp ./kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading and unzipping data\n",
        "Designed for kaggle and google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vK_RZzQPuS4",
        "outputId": "62f5ac4e-117e-4cd2-d91f-d08ffd77add7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading automatic-vote-counting-image.zip to /content\n",
            " 99% 2.38G/2.40G [00:26<00:00, 76.6MB/s]\n",
            "100% 2.40G/2.40G [00:26<00:00, 98.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "! kaggle competitions download -c \"automatic-vote-counting-image\"\n",
        "! apt install unzip\n",
        "!unzip automatic-vote-counting-image \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yE43xY18TWbe",
        "outputId": "fb4f1b0f-147d-47d0-aeb8-5030c809b7bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-335cf2e6-f6bb-4cda-97c0-1192deba5b61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12_248.jpeg</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39_1627.jpeg</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22_536.jpeg</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33_612.jpeg</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1_1158.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-335cf2e6-f6bb-4cda-97c0-1192deba5b61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-335cf2e6-f6bb-4cda-97c0-1192deba5b61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-335cf2e6-f6bb-4cda-97c0-1192deba5b61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           data  label\n",
              "0   12_248.jpeg     12\n",
              "1  39_1627.jpeg     39\n",
              "2   22_536.jpeg     22\n",
              "3   33_612.jpeg     33\n",
              "4   1_1158.jpeg      1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "base_dir = \"./IMAGE DATASET/train\"\n",
        "current_dir = \"./train\"\n",
        "\n",
        "labels_path = \"./IMAGE DATASET/train.csv\"\n",
        "train = pd.read_csv(labels_path)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDFqhpTctQQ_"
      },
      "outputs": [],
      "source": [
        "# !mkdir train\n",
        "!mkdir train_split\n",
        "!mkdir validation_split\n",
        "# !for i in {0..47}; do mkdir \"train/$i\"; done\n",
        "!for i in {0..47}; do mkdir \"train_split/$i\"; done\n",
        "!for i in {0..47}; do mkdir \"validation_split/$i\"; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soz3DtLLtSF0",
        "outputId": "bcc1a850-d399-43f3-c233-b9bbf1b63ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "5000\n",
            "10000\n",
            "15000\n",
            "20000\n",
            "25000\n",
            "30000\n",
            "35000\n",
            "40000\n",
            "45000\n",
            "50000\n",
            "55000\n",
            "60000\n",
            "65000\n",
            "70000\n",
            "75000\n",
            "80000\n",
            "85000\n",
            "90000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "for i in range(91960):\n",
        "    num = random.random()\n",
        "    current_dir = \"./train_split\" if num < 0.9 else \"./validation_split\"\n",
        "    file_name = train.iloc[i,0]\n",
        "    label = train.iloc[i, 1]\n",
        "    dst = os.path.join(current_dir, str(label))\n",
        "    src = os.path.join(base_dir, file_name)\n",
        "    shutil.copy(src, dst)\n",
        "    \n",
        "    if i % 5000 == 0:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U_dn8QmuPM-",
        "outputId": "2881ee00-793e-41c7-9d10-f84415485f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 82845 images belonging to 48 classes.\n",
            "Found 9115 images belonging to 48 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "input_shape = [96, 96]\n",
        "batch_size = 256\n",
        "\n",
        "TRAINING_DIR = \"./train_split\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      rotation_range=10,\n",
        "      shear_range=0.1,\n",
        "      )\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(input_shape[0],input_shape[1]),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=batch_size\n",
        ")\n",
        "VALIDATION_DIR = \"./validation_split\"\n",
        "validation_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      )\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(input_shape[0],input_shape[1]),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9WozHXq1-A-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "train_epoches = np.ceil(82814 / batch_size)\n",
        "val_epoches = np.ceil(9146 / batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjOyBGpJu_hO"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#     # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "#     # This is the first convolution\n",
        "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(212, 309, 3)),\n",
        "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#     # The second convolution\n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     # The third convolution\n",
        "#     tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     # The fourth convolution\n",
        "#     # Flatten the results to feed into a DNN\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     # tf.keras.layers.Dropout(0.5),\n",
        "#     # 512 neuron hidden layer\n",
        "#     tf.keras.layers.Dense(128, activation='relu'),\n",
        "#     tf.keras.layers.Dense(128, activation='relu'),\n",
        "#     tf.keras.layers.Dense(48, activation='softmax')\n",
        "# ])\n",
        "# model.build()\n",
        "\n",
        "# # Print the model summary\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqPSW4rrDDaG"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#     # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "#     # This is the first convolution\n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(212, 309, 3)),\n",
        "#     tf.keras.layers.MaxPooling2D(2, 2),\n",
        "#     # The second convolution\n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     # The third convolution\n",
        "#     tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     # The fourth convolution\n",
        "#     tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     # Flatten the results to feed into a DNN\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     # tf.keras.layers.Dropout(0.5),\n",
        "#     # 512 neuron hidden layer\n",
        "#     tf.keras.layers.Dense(128, activation='relu'),\n",
        "#     tf.keras.layers.Dense(128, activation='relu'),\n",
        "#     tf.keras.layers.Dense(48, activation='softmax')\n",
        "# ])\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okcEvt-85Si4",
        "outputId": "4ddefba0-a5a7-4218-c9d5-7ca1700e2ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 94, 94, 32)        896       \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 92, 92, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 92, 92, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 46, 46, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 46, 46, 32)        0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 44, 44, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 44, 44, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 42, 42, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 42, 42, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 21, 21, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 19, 19, 128)       147584    \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 17, 17, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 17, 17, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 8, 8, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 16384)             0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 16384)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               2097280   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 48)                6192      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,650,640\n",
            "Trainable params: 2,649,680\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(input_shape[0], input_shape[1], 3)),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(input_shape[0], input_shape[1], 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(48, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j-ThvLevGyB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.0001,\n",
        "), metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJF--TxvLqm",
        "outputId": "38e2b1fd-d0ed-45f6-a9f2-daf1e9a66aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 3.8388 - accuracy: 0.0287\n",
            "Epoch 1: val_accuracy improved from -inf to 0.02644, saving model to model.h5\n",
            "324/324 [==============================] - 520s 2s/step - loss: 3.8388 - accuracy: 0.0287 - val_loss: 34.8819 - val_accuracy: 0.0264\n",
            "Epoch 2/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.4323 - accuracy: 0.3785\n",
            "Epoch 2: val_accuracy improved from 0.02644 to 0.80230, saving model to model.h5\n",
            "324/324 [==============================] - 532s 2s/step - loss: 2.4323 - accuracy: 0.3785 - val_loss: 0.5574 - val_accuracy: 0.8023\n",
            "Epoch 3/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9761\n",
            "Epoch 3: val_accuracy did not improve from 0.80230\n",
            "324/324 [==============================] - 534s 2s/step - loss: 0.1129 - accuracy: 0.9761 - val_loss: 0.8963 - val_accuracy: 0.7244\n",
            "Epoch 4/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9841\n",
            "Epoch 4: val_accuracy improved from 0.80230 to 0.98705, saving model to model.h5\n",
            "324/324 [==============================] - 535s 2s/step - loss: 0.0712 - accuracy: 0.9841 - val_loss: 0.0595 - val_accuracy: 0.9871\n",
            "Epoch 5/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9867\n",
            "Epoch 5: val_accuracy improved from 0.98705 to 0.98837, saving model to model.h5\n",
            "324/324 [==============================] - 528s 2s/step - loss: 0.0574 - accuracy: 0.9867 - val_loss: 0.0457 - val_accuracy: 0.9884\n",
            "Epoch 6/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9887\n",
            "Epoch 6: val_accuracy did not improve from 0.98837\n",
            "324/324 [==============================] - 529s 2s/step - loss: 0.0478 - accuracy: 0.9887 - val_loss: 0.0763 - val_accuracy: 0.9827\n",
            "Epoch 7/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9903\n",
            "Epoch 7: val_accuracy did not improve from 0.98837\n",
            "324/324 [==============================] - 528s 2s/step - loss: 0.0392 - accuracy: 0.9903 - val_loss: 0.0826 - val_accuracy: 0.9832\n",
            "Epoch 8/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9922\n",
            "Epoch 8: val_accuracy improved from 0.98837 to 0.99046, saving model to model.h5\n",
            "324/324 [==============================] - 526s 2s/step - loss: 0.0309 - accuracy: 0.9922 - val_loss: 0.0379 - val_accuracy: 0.9905\n",
            "Epoch 9/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9930\n",
            "Epoch 9: val_accuracy did not improve from 0.99046\n",
            "324/324 [==============================] - 530s 2s/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0743 - val_accuracy: 0.9750\n",
            "Epoch 10/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9937\n",
            "Epoch 10: val_accuracy did not improve from 0.99046\n",
            "324/324 [==============================] - 530s 2s/step - loss: 0.0234 - accuracy: 0.9937 - val_loss: 1.8953 - val_accuracy: 0.4405\n",
            "Epoch 11/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9951\n",
            "Epoch 11: val_accuracy did not improve from 0.99046\n",
            "324/324 [==============================] - 528s 2s/step - loss: 0.0200 - accuracy: 0.9951 - val_loss: 18.1588 - val_accuracy: 0.0302\n",
            "Epoch 12/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9957\n",
            "Epoch 12: val_accuracy improved from 0.99046 to 0.99616, saving model to model.h5\n",
            "324/324 [==============================] - 527s 2s/step - loss: 0.0176 - accuracy: 0.9957 - val_loss: 0.0195 - val_accuracy: 0.9962\n",
            "Epoch 13/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9960\n",
            "Epoch 13: val_accuracy did not improve from 0.99616\n",
            "324/324 [==============================] - 529s 2s/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 0.0211 - val_accuracy: 0.9951\n",
            "Epoch 14/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9969\n",
            "Epoch 14: val_accuracy did not improve from 0.99616\n",
            "324/324 [==============================] - 524s 2s/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.2930 - val_accuracy: 0.9003\n",
            "Epoch 15/15\n",
            "324/324 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9967\n",
            "Epoch 15: val_accuracy improved from 0.99616 to 0.99649, saving model to model.h5\n",
            "324/324 [==============================] - 527s 2s/step - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.0164 - val_accuracy: 0.9965\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_generator, epochs=15, steps_per_epoch=train_epoches, verbose = 1, \n",
        "                    validation_data = validation_generator, validation_steps=val_epoches,\n",
        "                    callbacks=[checkpoint]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_SV3MVStDE3"
      },
      "outputs": [],
      "source": [
        "# !mkdir train_separated\n",
        "# !mkdir validation\n",
        "\n",
        "# import random\n",
        "\n",
        "# !for i in {0..47}; do mkdir \"train_separated/$i\"; done\n",
        "# !for i in {0..47}; do mkdir \"validation/$i\"; done\n",
        "# for i in range(91960):\n",
        "\n",
        "#   num = random.random()\n",
        "#   current_dir = \"./train_separated\" if num < 0.8 else \"./validation\"\n",
        "#   file_name = train.iloc[i,0]\n",
        "#   label = train.iloc[i, 1]\n",
        "\n",
        "#   dst = os.path.join(current_dir, str(label))\n",
        "#   src = os.path.join(base_dir, file_name)\n",
        "#   shutil.copy(src, dst)\n",
        "    \n",
        "#   if i % 5000 == 0:\n",
        "#     print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZry-IEvBwm_"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
        "\n",
        "print(tf.version.VERSION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdxBCIw8S8Wn"
      },
      "outputs": [],
      "source": [
        "!mkdir ./IMAGE\\ DATASET/test/0\n",
        "!cp ./IMAGE\\ DATASET/test/*.jpeg ./IMAGE\\ DATASET/test/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rku2PE2bPsL3",
        "outputId": "9bf9fd6e-139c-4936-a4d6-d0bfb0eac6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4840 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      )\n",
        "test_data = \"./IMAGE DATASET/test\"\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "\ttest_data,\n",
        "\ttarget_size=(input_shape[0],input_shape[1]),\n",
        "  batch_size=1,\n",
        "\tclass_mode='categorical',\n",
        "  shuffle=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ifEhn4zQrZN",
        "outputId": "d42981db-9ffb-4a6a-e346-93d2f1bfe522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4840/4840 [==============================] - 34s 7ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[5.57053974e-03, 9.94429350e-01, 1.06522587e-19, ...,\n",
              "        1.64894370e-13, 4.26555357e-14, 1.77321922e-18],\n",
              "       [5.43365240e-05, 4.55202144e-20, 2.40375485e-22, ...,\n",
              "        5.99523903e-13, 1.71510704e-14, 2.00006952e-16],\n",
              "       [1.20160446e-04, 2.00225791e-16, 9.47641105e-18, ...,\n",
              "        9.99764621e-01, 3.95895768e-05, 3.78478851e-13],\n",
              "       ...,\n",
              "       [5.34295663e-02, 1.77294640e-25, 5.96401292e-13, ...,\n",
              "        7.56190357e-20, 4.55967047e-16, 3.67021257e-12],\n",
              "       [2.50531656e-07, 2.29014256e-28, 8.69629908e-25, ...,\n",
              "        3.11408553e-17, 2.35799523e-28, 1.66009350e-30],\n",
              "       [3.14785848e-06, 2.90077061e-30, 1.17944055e-29, ...,\n",
              "        4.21899115e-13, 3.74597858e-23, 3.75587270e-27]], dtype=float32)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred=model.predict(test_generator,verbose=1)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "predicted_class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awb3J8MRukD5"
      },
      "outputs": [],
      "source": [
        "sequence_map = {\n",
        "    0: 0,\n",
        "    1: 1,\n",
        "    2: 10,\n",
        "    3: 11,\n",
        "    4: 12,\n",
        "    5: 13,\n",
        "    6: 14,\n",
        "    7: 15,\n",
        "    8: 16,\n",
        "    9: 17,\n",
        "    10: 18,\n",
        "    11: 19,\n",
        "    12: 2,\n",
        "    13: 20,\n",
        "    14: 21,\n",
        "    15: 22,\n",
        "    16: 23,\n",
        "    17: 24,\n",
        "    18: 25,\n",
        "    19: 26,\n",
        "    20: 27,\n",
        "    21: 28,\n",
        "    22: 29,\n",
        "    23: 3,\n",
        "    24: 30,\n",
        "    25: 31,\n",
        "    26: 32,\n",
        "    27: 33,\n",
        "    28: 34,\n",
        "    29: 35,\n",
        "    30: 36,\n",
        "    31: 37,\n",
        "    32: 38,\n",
        "    33: 39,\n",
        "    34: 4,\n",
        "    35: 40,\n",
        "    36: 41,\n",
        "    37: 42,\n",
        "    38: 43,\n",
        "    39: 44,\n",
        "    40: 45,\n",
        "    41: 46,\n",
        "    42: 47,\n",
        "    43: 5,\n",
        "    44: 6,\n",
        "    45: 7,\n",
        "    46: 8,\n",
        "    47: 9\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ03AMsRsoDQ"
      },
      "outputs": [],
      "source": [
        "# model2 = tf.keras.models.load_model(\"model_first.h5\")\n",
        "# pred=model2.predict_generator(test_generator,verbose=1)\n",
        "# predicted_class_indices=np.argmax(pred,axis=1)\n",
        "# predicted_class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s0oat6eVt7h"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "i = 0\n",
        "for file_name_path in test_generator.filenames:\n",
        "  file_name = file_name_path[2:]\n",
        "  data.append([file_name, sequence_map[predicted_class_indices[i]]])\n",
        "  i = i + 1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L3GVYionWPtk",
        "outputId": "d5943592-a25a-4876-dd36-a78c8488b9fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-43e04aaa-a892-44b7-98af-a61945a560fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_1509_422.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_2103_738.jpeg</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_2197_758.jpeg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_2316_1877.jpeg</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_2709_1014.jpeg</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43e04aaa-a892-44b7-98af-a61945a560fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43e04aaa-a892-44b7-98af-a61945a560fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43e04aaa-a892-44b7-98af-a61945a560fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               data  label\n",
              "0   0_1509_422.jpeg      1\n",
              "1   0_2103_738.jpeg     19\n",
              "2   0_2197_758.jpeg      7\n",
              "3  0_2316_1877.jpeg     19\n",
              "4  0_2709_1014.jpeg     37"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.DataFrame(data, columns=[\"data\", \"label\"])\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i68OZwMZRKw"
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"results.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4xlUj7QgIM9",
        "outputId": "4feb637b-9aaa-4d78-f5a6-02c1d7a5948b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 94.2k/94.2k [00:02<00:00, 45.1kB/s]\n",
            "Successfully submitted to IMAGE [IT Meet '22]"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c automatic-vote-counting-image -f results.csv -m \"Fixed mapping\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "-ZJghf0vgauG",
        "outputId": "cad854ce-f325-4366-a231-b44eca56491d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH7JjO4xzrOa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
